ğŸš€ POC Report: Transformer-Based Sequence Analysis for Customer Interaction Prediction

1ï¸âƒ£ Introduction

This Proof of Concept (POC) explores the use of a Transformer-based model to analyze customer journeys and predict whether an interaction occurs at some point in the sequence.
By modeling the sequence of steps a customer takes, we can:
âœ… Identify patterns in customer journeys
âœ… Generate contextual embeddings for downstream applications
âœ… Help businesses optimize user experiences


---

2ï¸âƒ£ Problem Statement

Customers follow different paths while interacting with a product or service.
Some journeys result in an interaction (Step 51), while others end without interaction (Step 182).

Our goal is to train a sequence model that takes a customer journey as input and predicts whether an interaction will occur.
Additionally, we extract contextual embeddings that encode journey characteristics.


---

3ï¸âƒ£ Dataset Creation â€“ The Most Crucial Step

Understanding the Customer Journey

A customerâ€™s journey is represented as a sequence of steps they take over time.
Each step consists of:

Step ID: Represents the action taken (e.g., browsing, adding items to cart).

Duration: Time taken before moving to the next step.


Key Rules for Dataset Construction

ğŸ“Œ Step 51 is not included in the input sequence to prevent target leakage.
ğŸ“Œ If Step 51 is present, we split the sequence at that point and set target = 1.
ğŸ“Œ If Step 51 is absent, the sequence must end with Step 182, and we set target = 0.

Example 1: Journey With Interaction (Target = 1)

(Target = 1 because Step 51 was encountered and removed)

Example 2: Journey Without Interaction (Target = 0)

(Target = 0 because Step 51 was never encountered)

Why Is This Crucial?

âš ï¸ Incorrect dataset construction can cause data leakage, leading to misleading results.
âœ… By carefully handling Step 51, we ensure the model learns from the journey itself, not the presence of an interaction.


---

4ï¸âƒ£ Model Architecture â€“ High-Level Overview

graph TD
    A[Customer Journey Sequence] -->|Step IDs & Durations| B[Embedding Layer]
    B --> C[Positional Encoding]
    C --> D[Transformer Encoder]
    D --> E[Final Representation]
    E --> F[Fully Connected Layer]
    F --> G[Sigmoid Activation]
    G --> H[Prediction: Interaction or Not]
    
    style B fill:#f9f,stroke:#333,stroke-width:2px
    style D fill:#ccf,stroke:#333,stroke-width:2px
    style H fill:#fc9,stroke:#333,stroke-width:2px

Key Components Explained Simply

ğŸ“Œ Embedding Layer
ğŸ”¹ Converts Step IDs into meaningful numeric representations.
ğŸ”¹ Transforms duration values into the same space.

ğŸ“Œ Positional Encoding
ğŸ”¹ Adds temporal context (since order matters).
ğŸ”¹ Ensures model understands sequence dependencies.

ğŸ“Œ Transformer Encoder
ğŸ”¹ Captures relationships between steps in the journey.
ğŸ”¹ Helps detect patterns that lead to interaction or dropout.

ğŸ“Œ Final Classification Layer
ğŸ”¹ Uses learned journey representation to predict interaction likelihood.

ğŸ“Œ Embedding Extraction
ğŸ”¹ Generates vector representations for downstream analytics.


---

5ï¸âƒ£ Model Training & Evaluation

âœ… Training

Uses historical customer journey data.

Optimized with Binary Cross-Entropy Loss (since target is 0/1).


âœ… Evaluation

Accuracy of interaction prediction.

Analysis of embeddings for customer segmentation.


âœ… Interpretability

Use SHAP values or Integrated Gradients to explain why the model predicts interaction for a given journey.

Identify the most influential steps.



---

6ï¸âƒ£ Next Steps & Enhancements

1ï¸âƒ£ Hyperparameter tuning to optimize model performance.
2ï¸âƒ£ Testing on real-world data to validate findings.
3ï¸âƒ£ Deploying embeddings for advanced customer segmentation.


---

ğŸ¯ Summary

ğŸš€ Developed a sequence model to analyze customer journeys.
ğŸ“Š Handled dataset challenges to prevent leakage.
ğŸ” Extracted embeddings for further insights.
ğŸ’¡ Scalable approach for real-world applications.

Would you like any refinements or additional sections? ğŸš€

Dataset Creation - Handling Customer Journey Sequences

1ï¸âƒ£ Understanding the Raw Sequence Data

Each customer journey consists of a series of step IDs with associated durations representing time taken to transition between steps. The sequences vary in length and structure.

ğŸ“Œ Two Possible Outcomes for a Sequence:

Interaction Occurs: Step 51 appears (we set target = 1).

No Interaction: The sequence ends with Step 182 (we set target = 0).



---

2ï¸âƒ£ Raw Sequence Examples (Before Processing)

Example 1: A Journey That Results in Interaction

ğŸš¶ Raw Sequence:
| Step ID | Duration (Seconds) | |---------|--------------------| | 5       | 1.2                | | 12      | 3.4                | | 33      | 2.5                | | 51      | 0.0                | | 60      | 1.8                | | 72      | 2.3                |

ğŸ“Œ Preprocessing Steps:
1ï¸âƒ£ Detect Step 51 â†’ Target = 1
2ï¸âƒ£ Remove Step 51 and everything after it (Step 51 should not be used as input)
3ï¸âƒ£ Final Processed Sequence:
| Step ID | Duration (Seconds) | |---------|--------------------| | 5       | 1.2                | | 12      | 3.4                | | 33      | 2.5                |


---

Example 2: A Journey Without Interaction

ğŸš¶ Raw Sequence:
| Step ID | Duration (Seconds) | |---------|--------------------| | 7       | 0.8                | | 14      | 2.1                | | 26      | 1.5                | | 39      | 3.0                | | 182     | 0.0                |

ğŸ“Œ Preprocessing Steps:
1ï¸âƒ£ Check for Step 51 â†’ Not Present â†’ Target = 0
2ï¸âƒ£ Ensure Step 182 is the final step (indicating journey ended without interaction)
3ï¸âƒ£ Final Processed Sequence:
| Step ID | Duration (Seconds) | |---------|--------------------| | 7       | 0.8                | | 14      | 2.1                | | 26      | 1.5                | | 39      | 3.0                |


---

3ï¸âƒ£ Why Is This Preprocessing Crucial?

ğŸš« Avoids Data Leakage:

If we included Step 51 in the input, the model would directly learn that Step 51 = interaction, instead of learning the patterns leading up to it.


ğŸ“Š Ensures Meaningful Learning:

The model now predicts interaction likelihood based on the steps leading up to it, rather than the presence of Step 51 itself.



---

4ï¸âƒ£ Final Processed Dataset

âœ… Now the model receives step sequences and durations as input and learns the probability of interaction happening.



