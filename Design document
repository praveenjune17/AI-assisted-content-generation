🚀 POC Report: Transformer-Based Sequence Analysis for Customer Interaction Prediction

1️⃣ Introduction

This Proof of Concept (POC) explores the use of a Transformer-based model to analyze customer journeys and predict whether an interaction occurs at some point in the sequence.
By modeling the sequence of steps a customer takes, we can:
✅ Identify patterns in customer journeys
✅ Generate contextual embeddings for downstream applications
✅ Help businesses optimize user experiences


---

2️⃣ Problem Statement

Customers follow different paths while interacting with a product or service.
Some journeys result in an interaction (Step 51), while others end without interaction (Step 182).

Our goal is to train a sequence model that takes a customer journey as input and predicts whether an interaction will occur.
Additionally, we extract contextual embeddings that encode journey characteristics.


---

3️⃣ Dataset Creation – The Most Crucial Step

Understanding the Customer Journey

A customer’s journey is represented as a sequence of steps they take over time.
Each step consists of:

Step ID: Represents the action taken (e.g., browsing, adding items to cart).

Duration: Time taken before moving to the next step.


Key Rules for Dataset Construction

📌 Step 51 is not included in the input sequence to prevent target leakage.
📌 If Step 51 is present, we split the sequence at that point and set target = 1.
📌 If Step 51 is absent, the sequence must end with Step 182, and we set target = 0.

Example 1: Journey With Interaction (Target = 1)

(Target = 1 because Step 51 was encountered and removed)

Example 2: Journey Without Interaction (Target = 0)

(Target = 0 because Step 51 was never encountered)

Why Is This Crucial?

⚠️ Incorrect dataset construction can cause data leakage, leading to misleading results.
✅ By carefully handling Step 51, we ensure the model learns from the journey itself, not the presence of an interaction.


---

4️⃣ Model Architecture – High-Level Overview

graph TD
    A[Customer Journey Sequence] -->|Step IDs & Durations| B[Embedding Layer]
    B --> C[Positional Encoding]
    C --> D[Transformer Encoder]
    D --> E[Final Representation]
    E --> F[Fully Connected Layer]
    F --> G[Sigmoid Activation]
    G --> H[Prediction: Interaction or Not]
    
    style B fill:#f9f,stroke:#333,stroke-width:2px
    style D fill:#ccf,stroke:#333,stroke-width:2px
    style H fill:#fc9,stroke:#333,stroke-width:2px

Key Components Explained Simply

📌 Embedding Layer
🔹 Converts Step IDs into meaningful numeric representations.
🔹 Transforms duration values into the same space.

📌 Positional Encoding
🔹 Adds temporal context (since order matters).
🔹 Ensures model understands sequence dependencies.

📌 Transformer Encoder
🔹 Captures relationships between steps in the journey.
🔹 Helps detect patterns that lead to interaction or dropout.

📌 Final Classification Layer
🔹 Uses learned journey representation to predict interaction likelihood.

📌 Embedding Extraction
🔹 Generates vector representations for downstream analytics.


---

5️⃣ Model Training & Evaluation

✅ Training

Uses historical customer journey data.

Optimized with Binary Cross-Entropy Loss (since target is 0/1).


✅ Evaluation

Accuracy of interaction prediction.

Analysis of embeddings for customer segmentation.


✅ Interpretability

Use SHAP values or Integrated Gradients to explain why the model predicts interaction for a given journey.

Identify the most influential steps.



---

6️⃣ Next Steps & Enhancements

1️⃣ Hyperparameter tuning to optimize model performance.
2️⃣ Testing on real-world data to validate findings.
3️⃣ Deploying embeddings for advanced customer segmentation.


---

🎯 Summary

🚀 Developed a sequence model to analyze customer journeys.
📊 Handled dataset challenges to prevent leakage.
🔍 Extracted embeddings for further insights.
💡 Scalable approach for real-world applications.

Would you like any refinements or additional sections? 🚀

Dataset Creation - Handling Customer Journey Sequences

1️⃣ Understanding the Raw Sequence Data

Each customer journey consists of a series of step IDs with associated durations representing time taken to transition between steps. The sequences vary in length and structure.

📌 Two Possible Outcomes for a Sequence:

Interaction Occurs: Step 51 appears (we set target = 1).

No Interaction: The sequence ends with Step 182 (we set target = 0).



---

2️⃣ Raw Sequence Examples (Before Processing)

Example 1: A Journey That Results in Interaction

🚶 Raw Sequence:
| Step ID | Duration (Seconds) | |---------|--------------------| | 5       | 1.2                | | 12      | 3.4                | | 33      | 2.5                | | 51      | 0.0                | | 60      | 1.8                | | 72      | 2.3                |

📌 Preprocessing Steps:
1️⃣ Detect Step 51 → Target = 1
2️⃣ Remove Step 51 and everything after it (Step 51 should not be used as input)
3️⃣ Final Processed Sequence:
| Step ID | Duration (Seconds) | |---------|--------------------| | 5       | 1.2                | | 12      | 3.4                | | 33      | 2.5                |


---

Example 2: A Journey Without Interaction

🚶 Raw Sequence:
| Step ID | Duration (Seconds) | |---------|--------------------| | 7       | 0.8                | | 14      | 2.1                | | 26      | 1.5                | | 39      | 3.0                | | 182     | 0.0                |

📌 Preprocessing Steps:
1️⃣ Check for Step 51 → Not Present → Target = 0
2️⃣ Ensure Step 182 is the final step (indicating journey ended without interaction)
3️⃣ Final Processed Sequence:
| Step ID | Duration (Seconds) | |---------|--------------------| | 7       | 0.8                | | 14      | 2.1                | | 26      | 1.5                | | 39      | 3.0                |


---

3️⃣ Why Is This Preprocessing Crucial?

🚫 Avoids Data Leakage:

If we included Step 51 in the input, the model would directly learn that Step 51 = interaction, instead of learning the patterns leading up to it.


📊 Ensures Meaningful Learning:

The model now predicts interaction likelihood based on the steps leading up to it, rather than the presence of Step 51 itself.



---

4️⃣ Final Processed Dataset

✅ Now the model receives step sequences and durations as input and learns the probability of interaction happening.



